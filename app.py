from flask import Flask, request, jsonify
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os

app = Flask(__name__)

# ğŸ” æœ¬ç•ªç”¨ï¼šAPIã‚­ãƒ¼ã¯Renderã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
openai_api_key = os.getenv("OPENAI_API_KEY")

# Chromaãƒ™ã‚¯ãƒˆãƒ«DBèª­ã¿è¾¼ã¿
embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedding)

# QAãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(openai_api_key=openai_api_key),
    retriever=vectorstore.as_retriever()
)

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾©
@app.route("/ask", methods=["POST"])
def ask():
    data = request.get_json()
    question = data.get("question", "")
    if not question:
        return jsonify({"error": "è³ªå•ãŒç©ºã§ã™"}), 400

    try:
        answer = qa_chain.run(question)
        return jsonify({"answer": answer})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# èµ·å‹•ï¼ˆRenderãŒè‡ªå‹•ã§èµ·å‹•ã—ã¦ãã‚Œã‚‹ï¼‰
if __name__ == "__main__":
    app.run()